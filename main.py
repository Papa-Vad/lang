import json
import re
import pymorphy3
from typing import List, Optional, Dict, Tuple
import uvicorn
from fastapi import FastAPI, Request, Depends, HTTPException
from pydantic import BaseModel
from contextlib import asynccontextmanager


class LawLink(BaseModel):
    law_id: Optional[int] = None
    article: Optional[str] = None
    point_article: Optional[str] = None
    subpoint_article: Optional[str] = None


class LinksResponse(BaseModel):
    links: List[LawLink]


class TextRequest(BaseModel):
    text: str


class LawReferenceParser:
    def __init__(self, codex_aliases: Dict):
        self.codex_aliases = codex_aliases
        self.morph = pymorphy3.MorphAnalyzer()
        
        # –õ–ï–ú–ú–ê–¢–ò–ó–ò–†–£–ï–ú –í–°–ï –Ω–∞–∑–≤–∞–Ω–∏—è –∑–∞–∫–æ–Ω–æ–≤ –∏–∑ –¥–∂–µ–π—Å–æ–Ω–∞
        self.law_name_to_id = {}
        all_law_names = []
        
        for law_id, names in codex_aliases.items():
            for name in names:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –ª–µ–º–∞—Ç–∏–∑–∞—Ü–∏—é
                normalized_name = self.normalize_law_name(name)
                self.law_name_to_id[normalized_name] = int(law_id)
                
                # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –¥–ª—è regex
                escaped_name = re.escape(normalized_name)
                all_law_names.append(escaped_name)
        
        # –ë–µ—Ä–µ–º –í–°–ï –ª–µ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        self.law_names_pattern = '|'.join(all_law_names)
        
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Å—ã–ª–æ–∫
        self.patterns = self._build_patterns()
    
    def normalize_law_name(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∑–∞–∫–æ–Ω–∞ —Å –ª–µ–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π"""
        if not text:
            return ""
        
        text = text.lower().strip()
        
        # –õ–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
        words = re.findall(r'\w+', text)
        normalized_words = []
        
        for word in words:
            try:
                parsed = self.morph.parse(word)[0]
                normalized_word = parsed.normal_form
                normalized_words.append(normalized_word)
            except Exception:
                normalized_words.append(word)
        
        return ' '.join(normalized_words)
    
    def normalize_text_references(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: –∑–∞–º–µ–Ω–∞ –ø–æ–ª–Ω—ã—Ö —Å–ª–æ–≤ –Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è"""
        # –ó–∞–º–µ–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ —Ñ–æ—Ä–º—ã –Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–µ
        replacements = [
            (r'\b–ø–æ–¥–ø–ø\.', '–ø–ø.'),
            (r'\b–ø—É–Ω–∫—Ç\w*\b', '–ø.'),
            (r'\b–ø—É–Ω—Ç\w*\b', '–ø.'),
            (r'\b–ø–æ–¥–ø—É–Ω–∫—Ç\w*\b', '–ø–ø.'),
            (r'\b—Å—Ç–∞—Ç—å\w+\b', '—Å—Ç.'),
            (r'\b—á–∞—Å—Ç—å\w*\b', '—á.'),
        ]
        
        normalized_text = text
        for pattern, replacement in replacements:
            normalized_text = re.sub(pattern, replacement, normalized_text, flags=re.IGNORECASE)
        
        return normalized_text
    
    def lemmatize_full_text(self, text: str) -> str:
        """–õ–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Å—Å—ã–ª–æ–∫"""
        # –°–Ω–∞—á–∞–ª–∞ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Å—ã–ª–∫–∏ (–∑–∞–º–µ–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è)
        normalized_text = self.normalize_text_references(text)
        
        # –ó–∞—Ç–µ–º –ª–µ–º–∞—Ç–∏–∑–∏—Ä—É–µ–º
        tokens = re.findall(r'\w+|\W+', normalized_text)
        lemmatized_tokens = []
        
        for token in tokens:
            # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–æ - –ª–µ–º–∞—Ç–∏–∑–∏—Ä—É–µ–º
            if re.match(r'^\w+$', token):
                try:
                    parsed = self.morph.parse(token)[0]
                    lemmatized_token = parsed.normal_form
                    lemmatized_tokens.append(lemmatized_token)
                except Exception:
                    lemmatized_tokens.append(token)
            else:
                # –ï—Å–ª–∏ –Ω–µ —Å–ª–æ–≤–æ (–ø—Ä–æ–±–µ–ª—ã, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è) - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                lemmatized_tokens.append(token)
        
        result = ''.join(lemmatized_tokens)
        return result
    
    def _build_patterns(self) -> List[Tuple[re.Pattern, str]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫"""
        patterns = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç–∞—Ç–µ–π (—Å –¥–µ—Ñ–∏—Å–∞–º–∏, —Ç–æ—á–∫–∞–º–∏ –∏ —Ç.–¥.)
        article_pattern = r'[\d]+(?:[\.\-‚Äì][\d]+)*'
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω: –ø–ø. X –ø. Y —Å—Ç. Z –ù–∞–∑–≤–∞–Ω–∏–µ (–ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
        pattern1 = re.compile(
            r'(?:–ø–ø\.\s*([^,\s]+(?:\s*[–∏,]\s*[^,\s]+)*))?\s*'  # –ø–æ–¥–ø—É–Ω–∫—Ç—ã
            r'(?:–ø\.\s*([^,\s]+(?:\s*[–∏,]\s*[^,\s]+)*))?\s*'   # –ø—É–Ω–∫—Ç—ã
            rf'(?:—Å—Ç\.\s*({article_pattern}))?\s*'              # —Å—Ç–∞—Ç—å—è
            fr'\s*({self.law_names_pattern})\b',                # –Ω–∞–∑–≤–∞–Ω–∏–µ
            re.IGNORECASE
        )
        patterns.append((pattern1, 'pattern1'))
        
        # –£–ü–†–û–©–ï–ù–ù–´–ô –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π - –∏–∑–±–µ–≥–∞–µ–º –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–≥–æ backtracking
        pattern2 = re.compile(
            rf'—Å—Ç\.\s*([\d\s\,\.\-‚Äì–∏]+?)\s+({self.law_names_pattern})\b',
            re.IGNORECASE
        )
        patterns.append((pattern2, 'pattern2'))
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø—Ä–æ—Å—Ç–æ–π —Å—Ç–∞—Ç—å–∏
        pattern3 = re.compile(
            rf'(?:—Å—Ç\.\s*({article_pattern}))\s*'
            fr'\s*({self.law_names_pattern})\b',
            re.IGNORECASE
        )
        patterns.append((pattern3, 'pattern3'))
        
        return patterns
    
    def parse_reference(self, match, pattern_type: str) -> Optional[List[LawLink]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å—Å—ã–ª–∫–∏ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç"""
        try:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            if not match.group():
                return None
                
            law_links = []
            
            if pattern_type == 'pattern1':
                subpoint_text = match.group(1)
                point_text = match.group(2)
                article_text = match.group(3)
                law_name = match.group(4)
            
            elif pattern_type == 'pattern2':
                subpoint_text = None
                point_text = None
                article_text = match.group(1)
                law_name = match.group(2)
            
            elif pattern_type == 'pattern3':
                subpoint_text = None
                point_text = None
                article_text = match.group(1)
                law_name = match.group(2)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ—Ç –∑–∞–∫–æ–Ω–∞
            if not law_name:
                return None
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º law_id
            law_id = None
            if law_name:
                normalized_law_name = law_name.lower().strip()
                law_id = self.law_name_to_id.get(normalized_law_name)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∑–∞–∫–æ–Ω –≤ —Å–ª–æ–≤–∞—Ä–µ
            if not law_id:
                return None
            
            # –û—á–∏—â–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            article = self.clean_component(article_text) if article_text else None
            point = self.clean_component(point_text) if point_text else None
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –±–µ–∑ –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ law_id)
            if not article and not point and not subpoint_text:
                return None
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ (–¥–ª—è pattern2)
            if pattern_type == 'pattern2' and article_text:
                articles = self.parse_multiple_articles(article_text)
                for art in articles:
                    law_link = LawLink(
                        law_id=law_id,
                        article=art,
                        point_article=None,
                        subpoint_article=None
                    )
                    law_links.append(law_link)
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥–ø—É–Ω–∫—Ç—ã
            elif subpoint_text:
                subpoints = self.parse_multiple_components(subpoint_text)
                for subpoint in subpoints:
                    law_link = LawLink(
                        law_id=law_id,
                        article=article,
                        point_article=point,
                        subpoint_article=subpoint
                    )
                    law_links.append(law_link)
            else:
                law_link = LawLink(
                    law_id=law_id,
                    article=article,
                    point_article=point,
                    subpoint_article=None
                )
                law_links.append(law_link)
        
            return law_links
            
        except Exception as e:
            return None

    def parse_multiple_articles(self, articles_text: str) -> List[str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–ª–æ–∂–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤"""
        if not articles_text:
            return []
        
        articles = re.findall(r'[\d]+(?:[\.\-‚Äì][\d]+)*', articles_text)
        return [art.strip() for art in articles if art.strip()]

    def parse_multiple_components(self, component: str) -> List[str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (–ø—É–Ω–∫—Ç–æ–≤, –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤)"""
        if not component:
            return []
        
        # –û—á–∏—â–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
        component = self.clean_component(component)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º, "–∏", –¥–µ—Ñ–∏—Å–∞–º, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        components = re.split(r'[,–∏;\s]+', component)
        
        # –û—á–∏—â–∞–µ–º –∫–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ
        cleaned_components = []
        for comp in components:
            comp = comp.strip()
            if comp and comp not in ['', '–∏', ',', ';']:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "1-3" -> ["1", "2", "3"])
                if re.match(r'^\d+\s*-\s*\d+$', comp):
                    range_parts = re.split(r'\s*-\s*', comp)
                    try:
                        start = int(range_parts[0])
                        end = int(range_parts[1])
                        for i in range(start, end + 1):
                            cleaned_components.append(str(i))
                    except:
                        cleaned_components.append(comp)
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±—É–∫–≤–µ–Ω–Ω—ã–µ –ø–æ–¥–ø—É–Ω–∫—Ç—ã (–∞, –±, –≤)
                elif re.match(r'^[–∞-—èa-z]$', comp, re.IGNORECASE):
                    cleaned_components.append(comp.lower())
                else:
                    cleaned_components.append(comp)
        
        return cleaned_components

    def clean_component(self, component: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å—Å—ã–ª–∫–∏ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not component:
            return None
        
        # –£–¥–∞–ª—è–µ–º —Å–ª–æ–≤–∞ "–ø—É–Ω–∫—Ç", "—Å—Ç–∞—Ç—å—è" –∏ —Ç.–¥. –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–∏—Å–ª–∞ –∏ –±—É–∫–≤—ã
        component = re.sub(r'\b(?:–ø|–ø—É–Ω–∫—Ç|—Å—Ç|—Å—Ç–∞—Ç—å—è|–ø–ø|–ø–æ–¥–ø—É–Ω–∫—Ç|—á|—á–∞—Å—Ç—å)\b\.?\s*', '', component, flags=re.IGNORECASE)
        
        # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–∏—Ñ—Ä—ã, –±—É–∫–≤—ã, —Ç–æ—á–∫–∏, –¥–µ—Ñ–∏—Å—ã
        component = re.sub(r'[^\w\s\.\-‚Äì‚Äî]', '', component)
        component = component.strip()
        
        return component if component else None

    def filter_redundant_references(self, references: List[LawLink]) -> List[LawLink]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ"""
        if not references:
            return []
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ —É—Ä–æ–≤–Ω—é –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ (–±–æ–ª—å—à–µ –Ω–µ-None –ø–æ–ª–µ–π = –≤—ã—à–µ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è)
        sorted_refs = sorted(references, key=lambda x: (
            x.subpoint_article is not None,  # –° –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º–∏ –≤—ã—à–µ (True > False)
            x.point_article is not None,     # –° –ø—É–Ω–∫—Ç–∞–º–∏ –≤—ã—à–µ
            x.article is not None           # –°–æ —Å—Ç–∞—Ç—å—è–º–∏ –≤—ã—à–µ
        ), reverse=True)
        
        filtered_references = []
        
        for ref in sorted_refs:
            is_redundant = False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—É—â–∞—è —Å—Å—ã–ª–∫–∞ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏
            for kept_ref in filtered_references:
                # –ï—Å–ª–∏ –∑–∞–∫–æ–Ω—ã —Ä–∞–∑–Ω—ã–µ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if ref.law_id != kept_ref.law_id:
                    continue
                    
                # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å–∏ —Ä–∞–∑–Ω—ã–µ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if ref.article != kept_ref.article:
                    continue
                
                # –°–ª—É—á–∞–π 1: –µ—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∞ —Å —Ç–µ–º –∂–µ –ø—É–Ω–∫—Ç–æ–º –∏ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–º - –¥—É–±–ª–∏–∫–∞—Ç
                if (kept_ref.point_article == ref.point_article and 
                    kept_ref.subpoint_article == ref.subpoint_article):
                    is_redundant = True
                    break
                
                # –°–ª—É—á–∞–π 2: –µ—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∞ —Å –ø–æ–¥–ø—É–Ω–∫—Ç–æ–º –¥–ª—è —Ç–æ–≥–æ –∂–µ –ø—É–Ω–∫—Ç–∞, —Ç–æ —Å—Å—ã–ª–∫–∞ —Ç–æ–ª—å–∫–æ —Å –ø—É–Ω–∫—Ç–æ–º –∏–∑–±—ã—Ç–æ—á–Ω–∞
                if (kept_ref.point_article == ref.point_article and 
                    kept_ref.subpoint_article is not None and 
                    ref.subpoint_article is None):
                    is_redundant = True
                    break
                
                # –°–ª—É—á–∞–π 3: –µ—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∞ —Å –ø—É–Ω–∫—Ç–æ–º –¥–ª—è —Ç–æ–π –∂–µ —Å—Ç–∞—Ç—å–∏, —Ç–æ —Å—Å—ã–ª–∫–∞ —Ç–æ–ª—å–∫–æ —Å–æ —Å—Ç–∞—Ç—å–µ–π –∏–∑–±—ã—Ç–æ—á–Ω–∞
                if (kept_ref.point_article is not None and 
                    ref.point_article is None and 
                    kept_ref.article == ref.article):
                    is_redundant = True
                    break
            
            if not is_redundant:
                filtered_references.append(ref)
        
        return filtered_references
    
    def extract_law_references(self, text: str) -> List[LawLink]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        references = []
        
        # –õ–ï–ú–ú–ê–¢–ò–ó–ò–†–£–ï–ú –í–ï–°–¨ –¢–ï–ö–°–¢ –î–û –ü–û–ò–°–ö–ê –†–ï–ì–£–õ–Ø–†–ö–ê–ú–ò
        lemmatized_text = self.lemmatize_full_text(text)
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        clean_text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', lemmatized_text)
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ –≤—Å–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤ –õ–ï–ú–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–û–ú —Ç–µ–∫—Å—Ç–µ
        for pattern, pattern_type in self.patterns:
            matches = pattern.finditer(clean_text)
            
            for match in matches:
                law_links = self.parse_reference(match, pattern_type)
                if law_links:
                    references.extend(law_links)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_references = []
        seen = set()
        for ref in references:
            key = (ref.law_id, ref.article, ref.point_article, ref.subpoint_article)
            if key not in seen:
                seen.add(key)
                unique_references.append(ref)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        filtered_references = self.filter_redundant_references(unique_references)
        
        return filtered_references


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    try:
        with open("law_aliases.json", "r", encoding="utf-8") as file:
            codex_aliases = json.load(file)
        
        app.state.codex_aliases = codex_aliases
        app.state.parser = LawReferenceParser(codex_aliases)
        print("üöÄ –°–µ—Ä–≤–∏—Å –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: {e}")
        raise
    yield
    # Shutdown
    print("üõë –°–µ—Ä–≤–∏—Å –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è...")


def get_parser(request: Request) -> LawReferenceParser:
    return request.app.state.parser


app = FastAPI(
    title="Law Links Detection API",
    description="–°–µ—Ä–≤–∏—Å –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
    version="1.0.0",
    lifespan=lifespan
)


@app.post("/detect")
async def get_law_links(
    data: TextRequest, 
    parser: LawReferenceParser = Depends(get_parser)
) -> LinksResponse:
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
        if len(data.text) > 50000:
            raise HTTPException(status_code=400, detail="Text too long")
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º –∫–∞–≤—ã—á–∫–∏
        cleaned_text = data.text.encode('utf-8', 'ignore').decode('utf-8')
        cleaned_text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', cleaned_text)
        # –ó–∞–º–µ–Ω—è–µ–º "–ø—Ä—è–º—ã–µ" –∫–∞–≤—ã—á–∫–∏ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ
        cleaned_text = cleaned_text.replace('"', '').replace('¬´', '').replace('¬ª', '')
        
        references = parser.extract_law_references(cleaned_text)
        return LinksResponse(links=references)
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
        return LinksResponse(links=[])


@app.get("/health")
async def health_check():
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞
    """
    return {"status": "healthy"}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8978)